INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Example rewards of one batch :[[(1.0, 0.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 0.6000000000000001, 1), (1.0, 0.0, 1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, -1.0, 1), (0.4285714285714286, 0.0, 1), (0.19999999999999996, -1.0, 1), (0.6000000000000001, 1.0, 1), (0.6000000000000001, 0.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -0.33333333333333337, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, -1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Example rewards of one batch :[[(1.0, 0.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 0.6000000000000001, 1), (1.0, 0.0, 1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, -1.0, 1), (0.4285714285714286, 0.0, 1), (0.19999999999999996, -1.0, 1), (0.6000000000000001, 1.0, 1), (0.6000000000000001, 0.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -0.33333333333333337, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, -1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Example rewards of one batch :[[(1.0, 0.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 0.6000000000000001, 1), (1.0, 0.0, 1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, -1.0, 1), (0.4285714285714286, 0.0, 1), (0.19999999999999996, -1.0, 1), (0.6000000000000001, 1.0, 1), (0.6000000000000001, 0.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -0.33333333333333337, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, -1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, -1), (1.0, -0.5, 1), (-0.1428571428571429, -1.0, 1)], [(1.0, 0.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.0, 1), (0.0, 1.0, 1), (0.0, 0.5, 1), (0.33333333333333326, 0.0, 1), (0.33333333333333326, 1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Example rewards of one batch :[[(1.0, 0.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 0.6000000000000001, 1), (1.0, 0.0, 1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, -1.0, 1), (0.4285714285714286, 0.0, 1), (0.19999999999999996, -1.0, 1), (0.6000000000000001, 1.0, 1), (0.6000000000000001, 0.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -0.33333333333333337, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, -1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, -1), (1.0, -0.5, 1), (-0.1428571428571429, -1.0, 1)], [(1.0, 0.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.0, 1), (0.0, 1.0, 1), (0.0, 0.5, 1), (0.33333333333333326, 0.0, 1), (0.33333333333333326, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.0, 1), (1.0, 0.0, 1), (1.0, 0.0, 1)], [(1.0, 0.19999999999999996, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1, 1.0, 1), (1.0, 1.0, 1), (0.33333333333333326, 1.0, 1), (0.0, 1.0, -1), (0.0, 1.0, 1), (0.0, 1.0, 1)], [(1.0, -0.5, 1), (0.33333333333333326, 0.0, -1), (0.6000000000000001, -1.0, 1), (0.7142857142857142, -0.33333333333333337, 1), (0.75, 1.0, 1), (0.75, 0.0, 1)], [(1.0, 1.0, 1), (1.0, 0.5, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, -1.0, 1), (1.0, 0.5, 1), (1.0, -0.5, -1), (1.0, -0.33333333333333337, 1), (0.7142857142857142, 0.0, 1), (0.7142857142857142, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1, 1.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (0.6666666666666667, -1.0, 1), (0.5384615384615385, 1.0, 1), (0.5384615384615385, -1.0, 1), (0.6923076923076923, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1)], [(1.0, -1.0, -1), (1.0, -1.0, 1), (1.0, 0.0, 1), (0.6000000000000001, 0.0, 1), (0.6000000000000001, 0.33333333333333326, 1), (1.0, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (0.75, -1.0, 1), (0.5, 0.33333333333333326, 1), (0.5, 1.0, 1)], [(1.0, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 0.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (0.8571428571428572, 0.33333333333333326, 1), (0.8571428571428572, -1.0, 1), (0.8571428571428572, -1.0, 1), (0.5294117647058822, -1.0, 1), (0.7647058823529411, 0.33333333333333326, 1)], [(1.0, 0.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -0.33333333333333337, 1), (0.6000000000000001, 1.0, 1), (0.5555555555555556, 0.0, 1), (0.7777777777777777, 0.0, 1), (0.7777777777777777, -1.0, 1), (0.7777777777777777, 0.0, 1), (0.8, -1.0, 1), (0.8, 1.0, 1), (0.5, 1.0, 1), (0.3846153846153846, 0.33333333333333326, 1), (0.3846153846153846, 0.0, 1), (0.3846153846153846, 1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Example rewards of one batch :[[(1.0, 0.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 0.6000000000000001, 1), (1.0, 0.0, 1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, -1.0, 1), (0.4285714285714286, 0.0, 1), (0.19999999999999996, -1.0, 1), (0.6000000000000001, 1.0, 1), (0.6000000000000001, 0.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -0.33333333333333337, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, -1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, -1), (1.0, -0.5, 1), (-0.1428571428571429, -1.0, 1)], [(1.0, 0.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.0, 1), (0.0, 1.0, 1), (0.0, 0.5, 1), (0.33333333333333326, 0.0, 1), (0.33333333333333326, 1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Example rewards of one batch :[[(1.0, -0.19999999999999996, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.7777777777777777, 0.0, 1), (0.7777777777777777, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, -1.0, 1), (0.6000000000000001, -0.5, 1), (0.6000000000000001, -0.5, 1), (0.6000000000000001, 0.33333333333333326, 1), (0.6000000000000001, 1.0, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (0.7142857142857142, 1.0, 1), (0.7142857142857142, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, -0.19999999999999996, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.7777777777777777, 0.0, 1), (0.7777777777777777, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, -1.0, 1), (0.6000000000000001, -0.5, 1), (0.6000000000000001, -0.5, 1), (0.6000000000000001, 0.33333333333333326, 1), (0.6000000000000001, 1.0, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (0.7142857142857142, 1.0, 1), (0.7142857142857142, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1)], [(1.0, -1.0, 1), (1.0, -0.5, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, -1.0, 1), (0.33333333333333326, -1.0, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (0.33333333333333326, -1.0, 1), (0.5, 0.33333333333333326, 1), (0.5, 1.0, -1), (0.5, -1.0, 1)], [(1.0, 0.5, 1), (1.0, -0.19999999999999996, 1), (1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1)], [(1.0, 0.0, 1), (1.0, -1.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1)], [(-1.0, 0.0, 1), (0.33333333333333326, 1.0, 1), (0.33333333333333326, -1.0, 1), (0.4285714285714286, 0.0, 1), (0.4285714285714286, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.0, 1), (0.6000000000000001, 0.33333333333333326, 1), (0.6000000000000001, 1.0, 1), (0.6000000000000001, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.5, 1), (1.0, -1.0, 1), (0.7142857142857142, 0.0, 1), (0.7142857142857142, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1)], [(1.0, -0.19999999999999996, 1), (1.0, -0.33333333333333337, 1), (0.7142857142857142, -1.0, 1), (0.7142857142857142, -1.0, 1), (0.7142857142857142, 1.0, 1)], [(1.0, 0.19999999999999996, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, -1.0, 1)], [(1.0, 1.0, 1), (1.0, -1.0, 1), (0.5, 0.6000000000000001, 1), (0.5, -1.0, 1), (0.5, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.0, 1), (1.0, 0.0, -1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (0.5, 0.0, 1), (0.5, 1.0, 1), (0.5, 0.0, 1), (0.4285714285714286, 0.0, 1)], [(1.0, 1.0, 1), (-0.19999999999999996, -1.0, 1), (0.0, 0.33333333333333326, 1), (0.0, 0.0, 1), (0.0, 0.0, 1)], [(1, -1.0, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, -1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.5, 0.33333333333333326, 1), (1.0, -1.0, 1)], [(1.0, 0.5, 1), (1.0, 0.5, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, -1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, -1.0, 1)], [(1.0, -1.0, 1), (0.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, -0.33333333333333337, 1)], [(1.0, 0.0, 1), (-0.19999999999999996, -1.0, 1), (0.6666666666666667, 0.0, 1), (0.6666666666666667, -1.0, 1), (0.6666666666666667, -1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Dialogs per rl epoch:8434
INFO:root:Example rewards of one batch :[[(1.0, -1.0, -1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, 1.0, 1)], [(1.0, -0.5, 1), (1.0, 0.19999999999999996, 1), (1.0, -1.0, 1)], [(1, 0.5, 1), (1, 0.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, -1.0, -1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, 1.0, 1)], [(1.0, -0.5, 1), (1.0, 0.19999999999999996, 1), (1.0, -1.0, 1)], [(1, 0.5, 1), (1, 0.0, 1), (1, 1.0, 1)], [(1.0, -1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -0.6, 1)], [(1.0, 1.0, 1), (1.0, -1.0, -1), (1.0, -1.0, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.33333333333333326, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1, 1.0, 1), (1, 1.0, 1), (1, 1.0, 1)], [(1.0, 0.0, -1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1, 0.33333333333333326, 1), (1, 1.0, 1), (1, 1.0, 1)], [(1, 0.0, 1), (1, 0.0, 1), (1, 1.0, 1)], [(1.0, 0.5, 1), (1.0, 1.0, 1), (1.0, -1.0, 1)], [(1.0, 0.0, -1), (1.0, -1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.19999999999999996, 1), (1.0, 1.0, 1), (1.0, 0.0, 1)], [(1.0, 0.33333333333333326, -1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1, 0.0, 1), (1, 0.0, 1), (1, -1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.19999999999999996, 1), (1.0, -0.19999999999999996, 1), (0.6666666666666667, -1.0, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1, -1.0, 1), (1, 0.0, 1), (1, -1.0, 1)], [(1.0, 0.5, -1), (1.0, 0.0, 1), (1.0, 1.0, 1)], [(1, 0.0, 1), (1, -1.0, 1), (1, 1.0, 1)], [(1.0, 0.33333333333333326, 1), (1.0, -0.33333333333333337, -1), (1.0, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, -1.0, -1), (1.0, -0.33333333333333337, 1), (1.0, 1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Dialogs per rl epoch:8434
INFO:root:Example rewards of one batch :[[(1.0, -1.0, -1), (0.6666666666666667, 1.0, 1), (0.6666666666666667, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, 1.0, 1)], [(1.0, -0.5, 1), (1.0, 0.19999999999999996, 1), (1.0, -1.0, 1)], [(1, 0.5, 1), (1, 0.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, -1.0, 1), (1.0, -0.33333333333333337, 1), (1.0, -0.6, 1)], [(1.0, 1.0, 1), (1.0, -1.0, -1), (1.0, -1.0, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 0.33333333333333326, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)]]
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Dialogs per rl epoch:1000
INFO:root:Example rewards of one batch :[[(1, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.5, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.19999999999999996, 1), (1.0, -0.19999999999999996, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.6666666666666667, 1), (1.0, 0.5, 1), (1.0, -1.0, 1)], [(1.0, 0.0, -1), (0.33333333333333326, -0.33333333333333337, 1), (0.33333333333333326, 1.0, -1), (0.33333333333333326, 0.0, 1)]]
INFO:root:Epoch:0, time:3.713 min
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Inference time:7.706133
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Inference time:7.709905
INFO:root:Initial dev reward:(5887.507702428299, 1123.4452380952378, 6481), metrics:(89.67935871742588, 81.66332665329843, 17.313419250037267, 102.98476193539943)
INFO:root:Dialogs per rl epoch:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Inference time:7.431229
INFO:root:Initial dev reward:(5887.507702428299, 1123.4452380952378, 6481), metrics:(89.67935871742588, 81.66332665329843, 17.313419250037267, 102.98476193539943)
INFO:root:Dialogs per rl epoch:1000
INFO:root:Example rewards of one batch :[[(1, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.5, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.19999999999999996, 1), (1.0, -0.19999999999999996, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.6666666666666667, 1), (1.0, 0.5, 1), (1.0, -1.0, 1)], [(1.0, 0.0, -1), (0.33333333333333326, -0.33333333333333337, 1), (0.33333333333333326, 1.0, -1), (0.33333333333333326, 0.0, 1)]]
INFO:root:Epoch:0, time:4.294 min
INFO:root:Inference time:7.668260
INFO:root:Average reward of epoch 0: 5.953063151145707
INFO:root:Epoch:1, time:3.779 min
INFO:root:Inference time:7.892291
INFO:root:Average reward of epoch 1: 5.451526966956655
INFO:root:Epoch:2, time:5.901 min
INFO:root:Inference time:7.843010
INFO:root:Average reward of epoch 2: 7.099350082330606
INFO:root:Epoch:3, time:4.175 min
INFO:root:Inference time:8.097765
INFO:root:Average reward of epoch 3: 5.700547412656788
INFO:root:Epoch:4, time:5.942 min
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Dialogs per rl epoch:1000
INFO:root:Example rewards of one batch :[[(1, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.5, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.19999999999999996, 1), (1.0, -0.19999999999999996, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.6666666666666667, 1), (1.0, 0.5, 1), (1.0, -1.0, 1)], [(1.0, 0.0, -1), (0.33333333333333326, -0.33333333333333337, 1), (0.33333333333333326, 1.0, -1), (0.33333333333333326, 0.0, 1)]]
INFO:root:Epoch:0, time:4.106 min
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Dialogs per rl epoch:1000
INFO:root:Example rewards of one batch :[[(1, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.5, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.19999999999999996, 1), (1.0, -0.19999999999999996, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.6666666666666667, 1), (1.0, 0.5, 1), (1.0, -1.0, 1)], [(1.0, 0.0, -1), (0.33333333333333326, -0.33333333333333337, 1), (0.33333333333333326, 1.0, -1), (0.33333333333333326, 0.0, 1)]]
INFO:root:Epoch:0, time:4.181 min
INFO:root:Inference time:7.347703
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Dialogs per rl epoch:1000
INFO:root:Example rewards of one batch :[[(1, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.5, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.19999999999999996, 1), (1.0, -0.19999999999999996, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.6666666666666667, 1), (1.0, 0.5, 1), (1.0, -1.0, 1)], [(1.0, 0.0, -1), (0.33333333333333326, -0.33333333333333337, 1), (0.33333333333333326, 1.0, -1), (0.33333333333333326, 0.0, 1)]]
INFO:root:Epoch:0, time:4.386 min
INFO:root:Inference time:8.015578
INFO:root:Dev reward:(5744.945464013113, 912.6325396825405, 6465), metrics:(85.57114228456057, 73.34669338676619, 15.584406970744514, 95.0433248064079)
INFO:root:Average reward of epoch 0: 5.953063151145707
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Dialogs per rl epoch:1000
INFO:root:Example rewards of one batch :[[(1, 0.0, 1), (1.0, 0.33333333333333326, 1), (1.0, -1.0, 1), (1.0, 0.33333333333333326, 1)], [(1.0, 0.0, 1), (1.0, 1.0, 1), (1.0, 0.33333333333333326, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 0.0, 1), (0.5, 1.0, 1)], [(1, 0.0, 1), (1, 1.0, 1), (1, -1.0, 1), (1, 1.0, 1)]]
INFO:root:Example rewards of one batch :[[(1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, 0.19999999999999996, 1), (1.0, -0.19999999999999996, 1), (1.0, 1.0, 1)], [(1.0, 1.0, 1), (1.0, -0.6666666666666667, 1), (1.0, 0.5, 1), (1.0, -1.0, 1)], [(1.0, 0.0, -1), (0.33333333333333326, -0.33333333333333337, 1), (0.33333333333333326, 1.0, -1), (0.33333333333333326, 0.0, 1)]]
INFO:root:Epoch:0, time:4.090 min
INFO:root:Inference time:7.421034
INFO:root:Dev reward:(5744.945464013113, 912.6325396825405, 6465), metrics:(85.57114228456057, 73.34669338676619, 15.584406970744514, 95.0433248064079)
INFO:root:Average reward of epoch 0: 5.953063151145707
INFO:root:Epoch:1, time:3.824 min
INFO:root:Inference time:7.347827
INFO:root:Dev reward:(5720.460793128447, 879.6238095238091, 6429), metrics:(84.969939879751, 73.84769539077416, 15.786179829153745, 95.19499746441633)
INFO:root:Average reward of epoch 1: 5.451526966956655
INFO:root:Epoch:2, time:5.697 min
INFO:root:Inference time:7.527195
INFO:root:Dev reward:(5750.722039072043, 971.6642857142847, 6464), metrics:(86.3727454909733, 75.15030060119487, 16.08296227804797, 96.84448532413205)
INFO:root:Average reward of epoch 2: 7.099350082330606
INFO:root:Epoch:3, time:3.757 min
INFO:root:Inference time:7.297680
INFO:root:Dev reward:(5747.377246772842, 919.8404761904766, 6426), metrics:(83.96793587173507, 74.24849699398052, 15.568557844014826, 94.67677427687264)
INFO:root:Average reward of epoch 3: 5.700547412656788
INFO:root:Epoch:4, time:5.635 min
INFO:root:Inference time:7.248121
INFO:root:Dev reward:(5697.409307359311, 947.2127705627706, 6432), metrics:(86.67334669337808, 76.2525050100124, 15.700684701361805, 97.16361055305705)
INFO:root:Average reward of epoch 4: 7.040145194719243
INFO:root:Epoch:5, time:4.372 min
INFO:root:Inference time:7.710540
INFO:root:Dev reward:(5746.9985902985945, 907.4785714285716, 6455), metrics:(85.67134268536215, 73.04609218436141, 16.108597652178634, 95.4673150870404)
INFO:root:Average reward of epoch 5: 5.765769064480897
INFO:root:Epoch:6, time:5.634 min
INFO:root:Inference time:7.546723
INFO:root:Dev reward:(5673.0613028354555, 857.5190476190484, 6471), metrics:(85.87174348696533, 74.7494989979885, 15.90108021575499, 96.2117014582319)
INFO:root:Average reward of epoch 6: 7.095673971693091
INFO:root:Epoch:7, time:4.668 min
INFO:root:Inference time:7.393443
INFO:root:Dev reward:(5671.682347391183, 814.4992063492059, 6464), metrics:(87.07414829658445, 76.1523046092108, 15.830116166177625, 97.44334261907525)
INFO:root:Average reward of epoch 7: 6.343745708055223
INFO:root:Epoch:8, time:4.361 min
INFO:root:Inference time:7.411338
INFO:root:Dev reward:(5679.4752570958435, 828.0119047619061, 6461), metrics:(84.36873747494144, 71.54308617233752, 15.759844166744704, 93.71575599038418)
INFO:root:Average reward of epoch 8: 6.116553858372275
INFO:root:Epoch:9, time:5.210 min
INFO:root:Inference time:7.583051
INFO:root:Dev reward:(5736.205920550046, 862.1714285714307, 6459), metrics:(85.37074148295737, 74.14829659317894, 16.065091955654225, 95.82461099372239)
INFO:root:Average reward of epoch 9: 6.752130162621026
INFO:root:Epoch:10, time:5.393 min
INFO:root:Inference time:7.400506
INFO:root:Dev reward:(5754.41394716395, 817.5246031746047, 6488), metrics:(85.0701402805526, 74.14829659317894, 16.017217736401452, 95.62643617326722)
INFO:root:Average reward of epoch 10: 6.689208954469508
INFO:root:Epoch:11, time:4.105 min
INFO:root:Inference time:7.568535
INFO:root:Dev reward:(5727.28081477347, 885.8976190476188, 6473), metrics:(86.17234468937012, 74.8496993987901, 16.039120625613947, 96.55014266969405)
INFO:root:Average reward of epoch 11: 5.938575484303019
INFO:root:Epoch:12, time:4.582 min
INFO:root:Inference time:7.559989
INFO:root:Dev reward:(5681.576401376405, 854.0634920634922, 6498), metrics:(85.0701402805526, 73.84769539077416, 15.949027035413312, 95.4079448710767)
INFO:root:Average reward of epoch 12: 6.01496182352723
INFO:root:Epoch:13, time:4.838 min
INFO:root:Inference time:7.824574
INFO:root:Dev reward:(5704.58202189967, 854.2166666666666, 6499), metrics:(85.77154308616375, 74.8496993987901, 16.016169333813032, 96.32679057628994)
INFO:root:Average reward of epoch 13: 6.149124264292289
INFO:root:Epoch:14, time:4.698 min
INFO:root:Inference time:7.643720
INFO:root:Dev reward:(5751.465479128715, 839.8809523809525, 6489), metrics:(86.77354709417968, 75.55110220440125, 15.891676229487715, 97.05400087877818)
INFO:root:Average reward of epoch 14: 6.296240418014888
INFO:root:Epoch:15, time:5.650 min
INFO:root:Inference time:7.388784
INFO:root:Dev reward:(5750.63575819605, 874.4238095238087, 6483), metrics:(86.3727454909733, 75.35070140279807, 16.021796160534585, 96.88351960742028)
INFO:root:Average reward of epoch 15: 7.03965782056964
INFO:root:Epoch:16, time:4.953 min
INFO:root:Inference time:7.455504
INFO:root:Dev reward:(5670.274885898419, 854.3357142857145, 6492), metrics:(87.47494989979083, 75.25050100199647, 16.08768948453075, 97.45041493542439)
INFO:root:Average reward of epoch 16: 6.610966129331249
INFO:root:Epoch:17, time:5.836 min
INFO:root:Inference time:7.401872
INFO:root:Dev reward:(5750.919384537031, 851.2428571428578, 6469), metrics:(84.86973947894941, 74.44889779558372, 16.199217646552995, 95.85853628381956)
INFO:root:Average reward of epoch 17: 7.022349732519257
INFO:root:Epoch:18, time:4.246 min
INFO:root:Inference time:7.553277
INFO:root:Dev reward:(5704.2926952132875, 819.9500000000008, 6406), metrics:(86.3727454909733, 74.94989979959169, 15.890255603112502, 96.551578248395)
INFO:root:Average reward of epoch 18: 5.9250522229124
INFO:root:Epoch:19, time:4.514 min
INFO:root:Inference time:7.663501
INFO:root:Dev reward:(5734.937117457711, 834.9412698412704, 6444), metrics:(85.87174348696533, 73.64729458917097, 16.08886534090669, 95.84838437897484)
INFO:root:Average reward of epoch 19: 6.300743156041005
INFO:root:Epoch:20, time:5.014 min
INFO:root:Inference time:7.702266
INFO:root:Dev reward:(5639.752458652462, 758.152380952381, 6451), metrics:(86.97394789578287, 74.64929859718691, 16.05854604288107, 96.87016928936595)
INFO:root:Average reward of epoch 20: 6.520579900561806
INFO:root:Epoch:21, time:5.472 min
INFO:root:Inference time:7.505826
INFO:root:Dev reward:(5700.842104137689, 813.3174603174599, 6479), metrics:(85.0701402805526, 74.8496993987901, 16.023285660895816, 95.98320550056715)
INFO:root:Average reward of epoch 21: 6.789333679589988
INFO:root:Epoch:22, time:4.255 min
INFO:root:Inference time:7.645100
INFO:root:Dev reward:(5723.5275530025565, 863.9642857142849, 6463), metrics:(84.969939879751, 73.64729458917097, 16.16493163463473, 95.47354886909571)
INFO:root:Average reward of epoch 22: 5.615352864896782
INFO:root:Epoch:23, time:5.282 min
INFO:root:Inference time:7.647137
INFO:root:Dev reward:(5651.256265956273, 848.1047619047623, 6480), metrics:(84.969939879751, 73.94789579157576, 16.090476887102852, 95.54939472276624)
INFO:root:Average reward of epoch 23: 6.5672694918184895
INFO:root:Epoch:24, time:4.407 min
INFO:root:Inference time:7.596403
INFO:root:Dev reward:(5701.139899642839, 813.728571428572, 6472), metrics:(85.57114228456057, 74.24849699398052, 16.160238552614693, 96.07005819188524)
INFO:root:Average reward of epoch 24: 6.008714751971255
INFO:root:Epoch:25, time:6.375 min
INFO:root:Inference time:7.679604
INFO:root:Dev reward:(5696.859552212489, 788.6166666666651, 6486), metrics:(85.27054108215579, 74.8496993987901, 15.98871173703112, 96.04883197750407)
INFO:root:Average reward of epoch 25: 7.2886663911202865
INFO:root:Epoch:26, time:4.900 min
INFO:root:Inference time:7.630906
INFO:root:Dev reward:(5717.610589737067, 816.9404761904765, 6487), metrics:(85.17034068135419, 73.44689378756779, 15.789170301577936, 95.09778753603894)
INFO:root:Average reward of epoch 26: 6.483584170278504
INFO:root:Epoch:27, time:4.977 min
INFO:root:Inference time:7.611255
INFO:root:Dev reward:(5688.967458358635, 843.3309523809535, 6460), metrics:(85.97194388776693, 75.15030060119487, 15.95262698528788, 96.51374922976878)
INFO:root:Average reward of epoch 27: 6.5781690947749665
INFO:root:Epoch:28, time:5.011 min
INFO:root:Inference time:7.559892
INFO:root:Dev reward:(5713.1204517704555, 791.8952380952385, 6498), metrics:(85.17034068135419, 74.14829659317894, 15.797668261465992, 95.45698689873255)
INFO:root:Average reward of epoch 28: 6.623115618658071
INFO:root:Epoch:29, time:4.144 min
INFO:root:Inference time:7.724562
INFO:root:Dev reward:(5706.197074820609, 876.5190476190488, 6508), metrics:(85.27054108215579, 72.64529058115504, 16.081587296313803, 95.03950312796921)
INFO:root:Average reward of epoch 29: 5.725024566298694
INFO:root:Epoch:30, time:4.985 min
INFO:root:Inference time:7.483249
INFO:root:Dev reward:(5721.623303493902, 806.4904761904755, 6469), metrics:(85.27054108215579, 73.54709418836937, 15.723781451966163, 95.13259908722874)
INFO:root:Average reward of epoch 30: 6.594127555154416
INFO:root:Epoch:31, time:4.990 min
INFO:root:Inference time:7.511170
INFO:root:Dev reward:(5692.285572760576, 742.5650793650802, 6489), metrics:(85.57114228456057, 74.54909819638532, 15.857833162852042, 95.91795340332499)
INFO:root:Average reward of epoch 31: 6.303888788641874
INFO:root:Epoch:32, time:4.319 min
INFO:root:Inference time:7.697634
INFO:root:Dev reward:(5780.977966331916, 864.1619047619064, 6459), metrics:(85.87174348696533, 74.7494989979885, 16.02642806453251, 96.33704930700942)
INFO:root:Average reward of epoch 32: 5.968745505525974
INFO:root:Epoch:33, time:5.127 min
INFO:root:Inference time:7.604405
INFO:root:Dev reward:(5777.959771927422, 879.9380952380953, 6496), metrics:(85.0701402805526, 73.94789579157576, 16.106438930272947, 95.61545696633712)
INFO:root:Average reward of epoch 33: 6.595966068464375
INFO:root:Epoch:34, time:4.194 min
INFO:root:Inference time:7.520062
INFO:root:Dev reward:(5667.540028925325, 820.9944444444449, 6475), metrics:(86.07214428856852, 75.25050100199647, 16.158556630239417, 96.81987927552191)
INFO:root:Average reward of epoch 34: 5.600417108718817
INFO:root:Epoch:35, time:4.788 min
INFO:root:Inference time:7.808526
INFO:root:Dev reward:(5711.825010610304, 836.6333333333333, 6472), metrics:(84.86973947894941, 72.74549098195664, 16.382961510442183, 95.19057674089521)
INFO:root:Average reward of epoch 35: 6.313488679225699
INFO:root:Epoch:36, time:4.896 min
INFO:root:Inference time:7.713431
INFO:root:Dev reward:(5690.13031968032, 774.645238095239, 6475), metrics:(86.4729458917749, 74.44889779558372, 16.224041633188644, 96.68496347686795)
INFO:root:Average reward of epoch 36: 6.724041217093262
INFO:root:Epoch:37, time:4.820 min
INFO:root:Inference time:7.927997
INFO:root:Dev reward:(5749.522836477253, 844.1476190476197, 6485), metrics:(85.97194388776693, 74.34869739478212, 15.956023116181175, 96.1163437574557)
INFO:root:Average reward of epoch 37: 6.287515572600801
INFO:root:Epoch:38, time:6.171 min
INFO:root:Inference time:7.434360
INFO:root:Dev reward:(5758.018491639084, 833.8619047619042, 6484), metrics:(86.17234468937012, 74.7494989979885, 15.92276956784252, 96.38369141152182)
INFO:root:Average reward of epoch 38: 7.639433221067135
INFO:root:Epoch:39, time:4.765 min
INFO:root:Inference time:7.478108
INFO:root:Dev reward:(5766.824709767359, 820.433333333333, 6451), metrics:(88.07615230460038, 75.65130260520284, 15.981697089045198, 97.8454245439468)
INFO:root:Average reward of epoch 39: 6.1509881638926505
