INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.tokenization_utils:Model name 'experiments_21/DS_base/best_score_model' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments_21/DS_base/best_score_model' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/vocab.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/merges.txt
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/tokenizer_config.json
INFO:transformers.tokenization_utils:Model name 'experiments_21/DS_base/best_score_model' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments_21/DS_base/best_score_model' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/vocab.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/merges.txt
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/tokenizer_config.json
INFO:transformers.tokenization_utils:Model name 'experiments_21/DS_base/best_score_model' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments_21/DS_base/best_score_model' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/vocab.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/merges.txt
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/tokenizer_config.json
INFO:transformers.tokenization_utils:Model name 'experiments_21/DS_base/best_score_model' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'experiments_21/DS_base/best_score_model' is a path, a model identifier, or url to a directory containing tokenizer files.
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/vocab.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/merges.txt
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/added_tokens.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/special_tokens_map.json
INFO:transformers.tokenization_utils:loading file experiments_21/DS_base/best_score_model/tokenizer_config.json
INFO:transformers.tokenization_utils:Assigning ['[restaurant]', '[hotel]', '[attraction]', '[train]', '[taxi]', '[police]', '[hospital]', '[general]', '[inform]', '[request]', '[nooffer]', '[recommend]', '[select]', '[offerbook]', '[offerbooked]', '[nobook]', '[bye]', '[greet]', '[reqmore]', '[welcome]', '[value_name]', '[value_choice]', '[value_area]', '[value_price]', '[value_type]', '[value_reference]', '[value_phone]', '[value_address]', '[value_food]', '[value_leave]', '[value_postcode]', '[value_id]', '[value_arrive]', '[value_stars]', '[value_day]', '[value_destination]', '[value_car]', '[value_departure]', '[value_time]', '[value_people]', '[value_stay]', '[value_pricerange]', '[value_department]', '[value_name]([value_phone]', '<pad>', '<go_r>', '<unk>', '<go_b>', '<go_a>', '<eos_u>', '<eos_r>', '<eos_b>', '<eos_a>', '<go_d>', '<eos_d>', '<sos_u>', '<sos_r>', '<sos_b>', '<sos_a>', '<sos_d>', '<sos_db>', '<eos_db>', '[db_nores]', '[db_0]', '[db_1]', '[db_2]', '[db_3]'] to the additional_special_tokens key of the tokenizer
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:transformers.configuration_utils:loading configuration file experiments_21/DS_base/best_score_model/config.json
INFO:transformers.configuration_utils:Model config GPT2Config {
  "_name_or_path": "../distilgpt2",
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 6,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "vocab_size": 50324
}

INFO:transformers.modeling_utils:loading weights file experiments_21/DS_base/best_score_model/pytorch_model.bin
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/new_db_se_blank_encoded.data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:Inference time:6.083 min
INFO:root:validation [CTR] 94.09  84.57  16.97  106.30  0.470
INFO:root:validation 91.48  82.77  16.97  104.09  0.470
INFO:root:Inference time:6.190 min
INFO:root:validation [CTR] 93.39  83.57  16.87  105.35  0.461
INFO:root:validation 90.98  81.96  16.87  103.34  0.461
INFO:root:Inference time:6.176 min
INFO:root:validation [CTR] 93.39  83.77  17.04  105.61  0.458
INFO:root:validation 89.98  80.86  17.04  102.46  0.458
