INFO:root:hotel database path:db/hotel_db_processed.json
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Reading encoded data from ./data/multi-woz-2.1-processed/encoded_us_data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:[50314, 50315, 50313, 50308, 50309, 50307]
INFO:root:Prior model loaded from ../distilgpt2
INFO:root:***** Running turn-level training *****
INFO:root:  Num Training steps(one turn in a batch of dialogs) per epoch = 4218
INFO:root:  Num Turns = 16868
INFO:root:  Num Dialogs = 8434
INFO:root:  Num Epochs = 50
INFO:root:  Batch size  = 4
INFO:root:  Gradient Accumulation steps = 8
INFO:root:  Total optimization steps = 26362
INFO:root:hotel database path:db/hotel_db_processed.json
INFO:root:Added special tokens to gpt tokenizer.
INFO:root:Encoding data now and save the encoded data in ./data/multi-woz-2.1-processed/encoded_us_data.json
INFO:root:encoded file saved in ./data/multi-woz-2.1-processed/encoded_us_data.json
INFO:root:train size:8434, dev size:999, test size:1000
INFO:root:[50314, 50315, 50313, 50308, 50309, 50307]
INFO:root:Prior model loaded from ../distilgpt2
INFO:root:***** Running turn-level training *****
INFO:root:  Num Training steps(one turn in a batch of dialogs) per epoch = 14195
INFO:root:  Num Turns = 56565
INFO:root:  Num Dialogs = 8421
INFO:root:  Num Epochs = 50
INFO:root:  Batch size  = 4
INFO:root:  Gradient Accumulation steps = 8
INFO:root:  Total optimization steps = 88718
